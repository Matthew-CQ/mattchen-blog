import{_ as t,c as n,I as s,j as e,a as o,a8 as r,D as i,o as l}from"./chunks/framework.BVyVpCZC.js";const P=JSON.parse('{"title":"无监督语义分割调研","description":"","frontmatter":{},"headers":[],"relativePath":"notes/Sementic Segmentation/无监督语义分割调研.md","filePath":"notes/Sementic Segmentation/无监督语义分割调研.md","lastUpdated":1726665992000}'),c={name:"notes/Sementic Segmentation/无监督语义分割调研.md"},g=e("h1",{id:"无监督语义分割调研",tabindex:"-1"},[o("无监督语义分割调研 "),e("a",{class:"header-anchor",href:"#无监督语义分割调研","aria-label":'Permalink to "无监督语义分割调研"'},"​")],-1),h=r('<h2 id="abstract" tabindex="-1">Abstract <a class="header-anchor" href="#abstract" aria-label="Permalink to &quot;Abstract&quot;">​</a></h2><p>半监督学习的核心问题在于有效利用无标注数据，作为有标签样本的补充，以提升模型性能。半监督分割的工作总结为两种：self-training和consistency learning。一般来说，self-training是离线处理的过程，而consistency learning是在线处理的。</p><p>Self-training</p><ol><li>Self-training主要分为3步_supervised learning → pseudo labeling → re-training_。第一步，我们在有标签数据上训练一个模型。第二步，我们用预训练好的模型，为无标签数据集生成伪标签。第三步，使用有标注数据集的真值标签，和无标注数据集的伪标签，重新训练一个模型。</li><li>Self-training可以通过pseudo labelling扩充数据集，在数据量不那么小的时候，性能非常的强。但是学生网络会从不正确的伪标签中学习到错误的信息，因而存在 performance degradation 的问题，需要通过强数据扩增 (SDA, strong data augmentation) 增加训练集的泛化性来打破这一局限。</li><li>通常作法是通过样本筛选等方式降低错误伪标签的影响，然而只选择高置信度的预测结果作为无标签样本的伪标签，这种朴素的 self-training 策略会将大量的无标签数据排除在训练过程外，导致模型训练不充分。此外，如果模型不能较好地预测某些 hard class，那么就很难为该类别的无标签像素分配准确的伪标签，从而进入恶性循环。</li></ol><p>Consistency learning Consistency learning的核心idea是：<strong>鼓励模型对经过不同变换的同一样本有相似的输出</strong>。这里“变换”包括高斯噪声、随机旋转、颜色的改变等等。<strong>consistency learning是如何提高模型效果的呢</strong>？在consistency learning中，我们通过对一个样本进行扰动（添加噪声等等），即改变了它在feature space中的位置。但我们希望模型对于改变之后的样本，预测出同样的类别。这个就会导致，在模型输出的特征空间中，同类别样本的特征靠的更近，而不同类别的特征离的更远。只有这样，扰动之后才不会让当前样本超出这个类别的覆盖范围。这也就导致学习出一个更加compact的特征编码。 Consistency learning基于两个假设：smoothness assumption 和 cluster assumption。 <strong>Smoothness assumption</strong>: samples close to each other are likely to have the same label. 靠的近的样本通常有相同的类别标签。 <strong>Cluster assumption</strong>: Decision boundary should lie in low-density regions of the data distribution. 模型预测的决策边界，通常处于样本分布密度低的区域。这个“密度低”指的是类别与类别之间的区域样本是比较稀疏的，那么一个好的决策边界应该尽可能处于这种样本稀疏的区域，这样才能更好地区分不同类别的样本。 Consistency learning主要有三类做法：Mean teacher， CPC， PseudoSeg。 Mean teacher， CPC， PseudoSeg，CPS简单介绍，图片来源：<a href="https://zhuanlan.zhihu.com/p/378120529" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/378120529</a> Mean-teacher: EMA介绍：指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。详见<a href="https://zhuanlan.zhihu.com/p/68748778" target="_blank" rel="noreferrer">解析</a>。</p><p>Mean teacher是17年提出的模型。给定一个输入图像X，添加不同的高斯噪声后得到X1和X2。我们将X1输入网络f(θ)中，得到预测P1；我们对f(θ)计算EMA，得到另一个网络，然后将X2输入这个EMA模型，得到另一个输出P2。最后，我们用P2作为P1的目标，用MSE loss约束。</p>',6);function d(_,p,m,u,f,S){const a=i("ArticleMetadata");return l(),n("div",null,[g,s(a),h])}const A=t(c,[["render",d]]);export{P as __pageData,A as default};
